{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet datasets transformers evaluate huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Download & cache into Colabâ€™s local disk\n",
    "dataset = load_dataset(\"ag_news\", download_mode=\"force_redownload\")\n",
    "\n",
    "train_ds = dataset[\"train\"]\n",
    "test_ds  = dataset[\"test\"]\n",
    "\n",
    "# Quick peek\n",
    "print(train_ds[0])\n",
    "print(train_ds.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn to DataFrame\n",
    "news_df = pd.DataFrame(train_ds)\n",
    "\n",
    "# counting how many examples per label\n",
    "counts = news_df[\"label\"].value_counts().sort_index()\n",
    "print(counts)\n",
    "\n",
    "label_names = train_ds.features[\"label\"].names\n",
    "print({label: label_names[label] for label in counts.index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def preprocess_text_simple(text):\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    # 2. Remove digits\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    # 3. Strip punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # 4. Split on whitespace\n",
    "    tokens = text.split()\n",
    "    # 5. Remove stopwords\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    # 6. Re-join\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Testing\n",
    "import pandas as pd\n",
    "sample = pd.DataFrame(train_ds.select(range(5)))\n",
    "sample[\"cleaned\"] = sample[\"text\"].apply(preprocess_text_simple)\n",
    "print(sample[[\"text\",\"cleaned\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_df[\"cleaned\"] = news_df[\"text\"].apply(preprocess_text_simple)\n",
    "\n",
    "\n",
    "print(news_df[[\"text\",\"cleaned\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
