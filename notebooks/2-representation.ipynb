{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/train.csv\n",
        "!wget -q https://raw.githubusercontent.com/mhjabreel/CharCnn_Keras/master/data/ag_news_csv/test.csv"
      ],
      "metadata": {
        "id": "yCQktpfRPeqb"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet huggingface_hub pandas datasets"
      ],
      "metadata": {
        "id": "bGLGzUyWxszC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load CSV\n",
        "train_df = pd.read_csv(\"train.csv\", header=None, names=[\"label\",\"title\",\"description\"])\n",
        "test_df  = pd.read_csv(\"test.csv\",  header=None, names=[\"label\",\"title\",\"description\"])\n",
        "\n",
        "# merge title+description\n",
        "train_df[\"text\"] = train_df[\"title\"].str.strip() + \". \" + train_df[\"description\"].str.strip()\n",
        "test_df[\"text\"]  = test_df[\"title\"].str.strip()  + \". \" + test_df[\"description\"].str.strip()"
      ],
      "metadata": {
        "id": "zs4gdgTqIZoH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizerFast\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Tokenize a small batch\n",
        "batch = list(train_df[\"text\"].iloc[:8])\n",
        "encoding = tokenizer(batch,\n",
        "                     padding=\"max_length\",\n",
        "                     truncation=True,\n",
        "                     max_length=128,\n",
        "                     return_tensors=\"pt\")\n",
        "\n",
        "print(\"input_ids:\", encoding[\"input_ids\"].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dpuQ3f1OWHD",
        "outputId": "7a661c86-38cd-4ec4-edfe-1d6a4b16e6b2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_ids: torch.Size([8, 128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "torch.save(encoding, \"tokenized_sample.pt\")\n"
      ],
      "metadata": {
        "id": "fHKn1kjGh0pC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1) (Re)install & upgrade to ensure a clean cache\n",
        "# !pip install --quiet --upgrade datasets transformers\n",
        "\n",
        "# # 2) Load AG News fresh into a Colab-local cache\n",
        "# from datasets import load_dataset\n",
        "\n",
        "# dataset = load_dataset(\n",
        "#     \"ag_news\",\n",
        "#     cache_dir=\"/tmp/hf_datasets\",       # Colabâ€™s ephemeral cache\n",
        "#     download_mode=\"force_redownload\"    # ignore any old cache\n",
        "# )\n",
        "# train_ds = dataset[\"train\"]\n",
        "# test_ds  = dataset[\"test\"]\n",
        "\n",
        "# # 3) Apply your tokenizer function over the full datasets\n",
        "# tokenized_train = train_ds.map(tokenize_batch, batched=True)\n",
        "# tokenized_test  = test_ds.map(tokenize_batch, batched=True)\n",
        "\n",
        "# # 4) Set the PyTorch format so Trainer can consume it directly\n",
        "# tokenized_train.set_format(\"torch\",\n",
        "#     columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "# tokenized_test.set_format(\"torch\",\n",
        "#     columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "# # 5) Quick sanity check on a tokenized example\n",
        "# print(tokenized_train[0])"
      ],
      "metadata": {
        "id": "I9mskr0-iWf0"
      },
      "execution_count": 8,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}